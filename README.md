# <a href='https://rnn-kitchen-project.streamlit.app' target="_blank">ğŸ” The RNN Kitchen </a>

**The RNN Kitchen** is an interactive **Streamlit app** for experimenting with Recurrent Neural Networks (RNNs) on text datasets.  
It lets you preprocess data, build/train models, and generate text â€” all from a clean browser-based UI.  

---

## ğŸš€ Features  

1. **Preprocess Data**  
   - Upload any `.txt` dataset.  
   - Configure preprocessing options (vocab size, padding type, lowercase, OOV tokens, etc.).  
   - Tokenizes and prepares sequences for model training.  

2. **Build & Train Models**  
   - Choose RNN type: `LSTM`, `GRU`, or `SimpleRNN`.  
   - Adjust architecture (embedding units, hidden layers, dropout).  
   - Pick optimizers and learning rate.  
   - Use callbacks like:  
     - **ModelCheckpoint** â€“ Save the best model.  
     - **EarlyStopping** â€“ Stop training when validation stops improving.  
     - **ReduceLROnPlateau** â€“ Adjust learning rate dynamically.  
   - View **real-time logs** of training inside the app.  

3. **Generate Text**  
   - Provide a **seed phrase**.  
   - Configure **temperature**, **Top-K**, or **Top-P** sampling.  
   - Generate realistic text based on your trained RNN.  

---

## ğŸ› ï¸ Installation  

Clone the repo and install dependencies:  

```bash
git clone https://github.com/your-username/rnn-kitchen.git
cd rnn-kitchen

# Create a virtual environment (recommended)
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows

# Install requirements
pip install -r requirements.txt
  ````

---

## â–¶ï¸ Usage

Run the app with:

```bash
streamlit run app.py
```

This will start the server, and you can access the app at:

```
http://localhost:8501
```

---

## ğŸ“‚ Project Structure

```
rnn-kitchen/
â”‚â”€â”€ app.py          # Streamlit frontend
â”‚â”€â”€ backend.py      # Core backend (data prep, training, text generation)
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ models/         # Saved trained models
â”‚â”€â”€ data/           # Uploaded datasets
```

---

## âš™ï¸ Configuration Parameters

### ğŸ”¹ Preprocessing

* **Vocab Size** â€“ Limit size of vocabulary.
* **OOV Token** â€“ Token for unseen words.
* **Padding Type** â€“ `pre` or `post`.
* **Lowercase** â€“ Convert text to lowercase.

### ğŸ”¹ Model Building

* **RNN Type** â€“ `LSTM`, `GRU`, or `SimpleRNN`.
* **Embedding Units** â€“ Size of embedding layer.
* **Hidden Layers** â€“ Number and size of recurrent layers.
* **Dropout** â€“ Prevents overfitting.
* **Optimizer** â€“ `Adam`, `RMSprop`, etc.
* **Learning Rate** â€“ Customizable.

### ğŸ”¹ Callbacks

* `ModelCheckpoint` â€“ Save the best model automatically.
* `EarlyStopping` â€“ Stop if validation loss doesnâ€™t improve.
* `ReduceLROnPlateau` â€“ Reduce learning rate on stagnation.

### ğŸ”¹ Text Generation

* **Seed Text** â€“ Starting phrase for generation.
* **Temperature** â€“ Controls creativity (higher = more random).
* **Top-K Sampling** â€“ Keep only top-K words at each step.
* **Top-P Sampling** â€“ Nucleus sampling with cumulative probability.

---

## ğŸ“Š Example: Generate Text

Once your model is trained, head to **Tab 3** (Generate Text) and try:

```text
Seed: "Once upon a time"
Temperature: 0.8
Top-K: 50
Output: "Once upon a time the king sat quietly in the garden, watching the sun fall behind the hills..."
```

---

## ğŸ§‘â€ğŸ’» Contributing

Pull requests are welcome!
If youâ€™d like to add new features (e.g., Transformer support), please fork and submit a PR.

---

## ğŸ“œ License

MIT License.
Feel free to use, modify, and share!


